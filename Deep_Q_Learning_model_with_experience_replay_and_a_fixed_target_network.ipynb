{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUbidLhL4V1TQJGHISvON5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dewwbe/Deep-learning-Lab-07-Part-2-Deep-Q-Learning-DQN-/blob/main/Deep_Q_Learning_model_with_experience_replay_and_a_fixed_target_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# ✅ Fixed dependency installation for LunarLander-v2 on Colab (Python 3.12)\n",
        "# ================================================================\n",
        "!apt-get install -y swig\n",
        "!pip install \"gymnasium[box2d]\" pygame box2d\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcNB59g71HkK",
        "outputId": "16342b1c-e45e-4a24-b58c-7ebe3d425970"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (779 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 126675 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.12/dist-packages (2.6.1)\n",
            "Collecting box2d\n",
            "  Downloading Box2D-2.3.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Using cached swig-4.3.1.post0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Using cached swig-4.3.1.post0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "Downloading Box2D-2.3.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp312-cp312-linux_x86_64.whl size=2381957 sha256=91a751ee6a72975886899796ffad3579b97481f7d33b4dba00264a4e99a13a69\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/e9/60/774da0bcd07f7dc7761a8590fa2d065e4069568e78dcdc3318\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py, box2d\n",
            "Successfully installed box2d-2.3.10 box2d-py-2.3.5 swig-4.3.1.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on:\", device)\n",
        "\n",
        "# Quick environment check\n",
        "env = gym.make(\"LunarLander-v3\")\n",
        "state, info = env.reset()\n",
        "print(\"✅ Environment ready, state shape:\", np.shape(state))\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hdh7s731QmF",
        "outputId": "fa6c8262-bfbe-465a-a1b1-8b073057e7d8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
            "<frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
            "<frozen importlib._bootstrap>:488: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n",
            "/usr/local/lib/python3.12/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Environment ready, state shape: (8,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 1. Neural Network Definition\n",
        "# ===============================\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "xnliDKtTxABo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 2. Replay Buffer\n",
        "# ===============================\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=100000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = map(np.array, zip(*batch))\n",
        "        return (\n",
        "            torch.FloatTensor(state).to(device),\n",
        "            torch.LongTensor(action).to(device),\n",
        "            torch.FloatTensor(reward).to(device),\n",
        "            torch.FloatTensor(next_state).to(device),\n",
        "            torch.FloatTensor(done).to(device),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n"
      ],
      "metadata": {
        "id": "0HlGjes7xDCW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 3. DQN Agent\n",
        "# ===============================\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, replay=True, target_update=True):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = 0.99\n",
        "        self.lr = 1e-3\n",
        "        self.batch_size = 64\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.replay_enabled = replay\n",
        "        self.target_update_enabled = target_update\n",
        "\n",
        "        self.online_net = DQN(state_dim, action_dim).to(device)\n",
        "        self.target_net = DQN(state_dim, action_dim).to(device)\n",
        "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
        "\n",
        "        self.optimizer = optim.Adam(self.online_net.parameters(), lr=self.lr)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "        self.memory = ReplayBuffer()\n",
        "\n",
        "        self.steps_done = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(self.action_dim)\n",
        "        state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        q_values = self.online_net(state_t)\n",
        "        return q_values.argmax().item()\n",
        "\n",
        "    def train_step(self):\n",
        "        if len(self.memory) < self.batch_size or not self.replay_enabled:\n",
        "            return 0.0\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "\n",
        "        q_values = self.online_net(states)\n",
        "        next_q_values = self.target_net(next_states)\n",
        "\n",
        "        q_target = q_values.clone()\n",
        "        for i in range(self.batch_size):\n",
        "            q_target[i, actions[i]] = rewards[i] + self.gamma * torch.max(next_q_values[i]) * (1 - dones[i])\n",
        "\n",
        "        loss = self.loss_fn(q_values, q_target.detach())\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        if self.target_update_enabled:\n",
        "            self.target_net.load_state_dict(self.online_net.state_dict())\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n"
      ],
      "metadata": {
        "id": "B39MNcrixGf_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===============================\n",
        "# 4. Training Function\n",
        "# ===============================\n",
        "def train_dqn(env_name=\"LunarLander-v2\", episodes=500, replay=True, target_update=True):\n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    agent = DQNAgent(state_dim, action_dim, replay, target_update)\n",
        "    rewards_history = []\n",
        "    eps_history = []\n",
        "    loss_history = []\n",
        "\n",
        "    target_update_freq = 1000\n",
        "    total_steps = 0\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            agent.memory.push(state, action, reward, next_state, done)\n",
        "\n",
        "            loss = agent.train_step()\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            agent.decay_epsilon()\n",
        "            eps_history.append(agent.epsilon)\n",
        "\n",
        "            if total_steps % target_update_freq == 0:\n",
        "                agent.update_target_network()\n",
        "\n",
        "            total_steps += 1\n",
        "\n",
        "        rewards_history.append(total_reward)\n",
        "        if ep % 10 == 0:\n",
        "            print(f\"Episode {ep}/{episodes} | Reward: {total_reward:.2f} | Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "    env.close()\n",
        "    return rewards_history, eps_history, loss_history"
      ],
      "metadata": {
        "id": "sMP3lofixKIe"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 5. Train and Compare\n",
        "# ===============================\n",
        "print(\"\\n=== Training DQN with Replay + Target Network ===\")\n",
        "rewards_replay, epsilons_replay, losses_replay = train_dqn(env_name=\"LunarLander-v3\", replay=True, target_update=True)\n",
        "\n",
        "print(\"\\n=== Training DQN without Replay/Target Network ===\")\n",
        "rewards_simple, epsilons_simple, losses_simple = train_dqn(env_name=\"LunarLander-v3\", replay=False, target_update=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwMIlHRVxO2O",
        "outputId": "fe279293-08c2-417e-e48c-dc9d6aa78fcc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training DQN with Replay + Target Network ===\n",
            "Episode 0/500 | Reward: -353.22 | Epsilon: 0.609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10/500 | Reward: -184.39 | Epsilon: 0.010\n",
            "Episode 20/500 | Reward: -1.93 | Epsilon: 0.010\n",
            "Episode 30/500 | Reward: -348.89 | Epsilon: 0.010\n",
            "Episode 40/500 | Reward: -657.84 | Epsilon: 0.010\n",
            "Episode 50/500 | Reward: -1685.14 | Epsilon: 0.010\n",
            "Episode 60/500 | Reward: -343.44 | Epsilon: 0.010\n",
            "Episode 70/500 | Reward: -239.27 | Epsilon: 0.010\n",
            "Episode 80/500 | Reward: -94.65 | Epsilon: 0.010\n",
            "Episode 90/500 | Reward: -104.27 | Epsilon: 0.010\n",
            "Episode 100/500 | Reward: -17.90 | Epsilon: 0.010\n",
            "Episode 110/500 | Reward: -247.59 | Epsilon: 0.010\n",
            "Episode 120/500 | Reward: -14.59 | Epsilon: 0.010\n",
            "Episode 130/500 | Reward: -39.13 | Epsilon: 0.010\n",
            "Episode 140/500 | Reward: -40.29 | Epsilon: 0.010\n",
            "Episode 150/500 | Reward: -136.22 | Epsilon: 0.010\n",
            "Episode 160/500 | Reward: -335.39 | Epsilon: 0.010\n",
            "Episode 170/500 | Reward: -188.88 | Epsilon: 0.010\n",
            "Episode 180/500 | Reward: -365.61 | Epsilon: 0.010\n",
            "Episode 190/500 | Reward: -148.19 | Epsilon: 0.010\n",
            "Episode 200/500 | Reward: -78.17 | Epsilon: 0.010\n",
            "Episode 210/500 | Reward: -216.62 | Epsilon: 0.010\n",
            "Episode 220/500 | Reward: -176.35 | Epsilon: 0.010\n",
            "Episode 230/500 | Reward: -218.06 | Epsilon: 0.010\n",
            "Episode 240/500 | Reward: -27.11 | Epsilon: 0.010\n",
            "Episode 250/500 | Reward: -95.46 | Epsilon: 0.010\n",
            "Episode 260/500 | Reward: -350.71 | Epsilon: 0.010\n",
            "Episode 270/500 | Reward: -382.66 | Epsilon: 0.010\n",
            "Episode 280/500 | Reward: -228.67 | Epsilon: 0.010\n",
            "Episode 290/500 | Reward: -199.06 | Epsilon: 0.010\n",
            "Episode 300/500 | Reward: -21.16 | Epsilon: 0.010\n",
            "Episode 310/500 | Reward: -301.61 | Epsilon: 0.010\n",
            "Episode 320/500 | Reward: 27.42 | Epsilon: 0.010\n",
            "Episode 330/500 | Reward: -403.26 | Epsilon: 0.010\n",
            "Episode 340/500 | Reward: -188.74 | Epsilon: 0.010\n",
            "Episode 350/500 | Reward: -169.39 | Epsilon: 0.010\n",
            "Episode 360/500 | Reward: -194.33 | Epsilon: 0.010\n",
            "Episode 370/500 | Reward: -77.19 | Epsilon: 0.010\n",
            "Episode 380/500 | Reward: -209.17 | Epsilon: 0.010\n",
            "Episode 390/500 | Reward: -173.48 | Epsilon: 0.010\n",
            "Episode 400/500 | Reward: -250.58 | Epsilon: 0.010\n",
            "Episode 410/500 | Reward: -215.39 | Epsilon: 0.010\n",
            "Episode 420/500 | Reward: -251.34 | Epsilon: 0.010\n",
            "Episode 430/500 | Reward: -144.38 | Epsilon: 0.010\n",
            "Episode 440/500 | Reward: -69.99 | Epsilon: 0.010\n",
            "Episode 450/500 | Reward: -228.60 | Epsilon: 0.010\n",
            "Episode 460/500 | Reward: -114.67 | Epsilon: 0.010\n",
            "Episode 470/500 | Reward: -151.19 | Epsilon: 0.010\n",
            "Episode 480/500 | Reward: -23.64 | Epsilon: 0.010\n",
            "Episode 490/500 | Reward: -178.35 | Epsilon: 0.010\n",
            "\n",
            "=== Training DQN without Replay/Target Network ===\n",
            "Episode 0/500 | Reward: -369.28 | Epsilon: 0.660\n",
            "Episode 10/500 | Reward: -759.01 | Epsilon: 0.017\n",
            "Episode 20/500 | Reward: -493.51 | Epsilon: 0.010\n",
            "Episode 30/500 | Reward: -442.72 | Epsilon: 0.010\n",
            "Episode 40/500 | Reward: -517.85 | Epsilon: 0.010\n",
            "Episode 50/500 | Reward: -641.79 | Epsilon: 0.010\n",
            "Episode 60/500 | Reward: -394.64 | Epsilon: 0.010\n",
            "Episode 70/500 | Reward: -835.00 | Epsilon: 0.010\n",
            "Episode 80/500 | Reward: -658.44 | Epsilon: 0.010\n",
            "Episode 90/500 | Reward: -597.26 | Epsilon: 0.010\n",
            "Episode 100/500 | Reward: -588.11 | Epsilon: 0.010\n",
            "Episode 110/500 | Reward: -485.06 | Epsilon: 0.010\n",
            "Episode 120/500 | Reward: -463.47 | Epsilon: 0.010\n",
            "Episode 130/500 | Reward: -424.89 | Epsilon: 0.010\n",
            "Episode 140/500 | Reward: -445.50 | Epsilon: 0.010\n",
            "Episode 150/500 | Reward: -487.21 | Epsilon: 0.010\n",
            "Episode 160/500 | Reward: -825.59 | Epsilon: 0.010\n",
            "Episode 170/500 | Reward: -457.96 | Epsilon: 0.010\n",
            "Episode 180/500 | Reward: -464.99 | Epsilon: 0.010\n",
            "Episode 190/500 | Reward: -635.16 | Epsilon: 0.010\n",
            "Episode 200/500 | Reward: -788.64 | Epsilon: 0.010\n",
            "Episode 210/500 | Reward: -588.48 | Epsilon: 0.010\n",
            "Episode 220/500 | Reward: -700.15 | Epsilon: 0.010\n",
            "Episode 230/500 | Reward: -549.57 | Epsilon: 0.010\n",
            "Episode 240/500 | Reward: -610.20 | Epsilon: 0.010\n",
            "Episode 250/500 | Reward: -517.10 | Epsilon: 0.010\n",
            "Episode 260/500 | Reward: -331.02 | Epsilon: 0.010\n",
            "Episode 270/500 | Reward: -520.76 | Epsilon: 0.010\n",
            "Episode 280/500 | Reward: -471.75 | Epsilon: 0.010\n",
            "Episode 290/500 | Reward: -926.24 | Epsilon: 0.010\n",
            "Episode 300/500 | Reward: -466.30 | Epsilon: 0.010\n",
            "Episode 310/500 | Reward: -464.58 | Epsilon: 0.010\n",
            "Episode 320/500 | Reward: -610.34 | Epsilon: 0.010\n",
            "Episode 330/500 | Reward: -585.98 | Epsilon: 0.010\n",
            "Episode 340/500 | Reward: -407.30 | Epsilon: 0.010\n",
            "Episode 350/500 | Reward: -455.03 | Epsilon: 0.010\n",
            "Episode 360/500 | Reward: -856.13 | Epsilon: 0.010\n",
            "Episode 370/500 | Reward: -441.98 | Epsilon: 0.010\n",
            "Episode 380/500 | Reward: -787.56 | Epsilon: 0.010\n",
            "Episode 390/500 | Reward: -455.59 | Epsilon: 0.010\n",
            "Episode 400/500 | Reward: -479.16 | Epsilon: 0.010\n",
            "Episode 410/500 | Reward: -552.81 | Epsilon: 0.010\n",
            "Episode 420/500 | Reward: -798.04 | Epsilon: 0.010\n",
            "Episode 430/500 | Reward: -465.07 | Epsilon: 0.010\n",
            "Episode 440/500 | Reward: -524.70 | Epsilon: 0.010\n",
            "Episode 450/500 | Reward: -438.71 | Epsilon: 0.010\n",
            "Episode 460/500 | Reward: -720.80 | Epsilon: 0.010\n",
            "Episode 470/500 | Reward: -724.50 | Epsilon: 0.010\n",
            "Episode 480/500 | Reward: -378.13 | Epsilon: 0.010\n",
            "Episode 490/500 | Reward: -551.15 | Epsilon: 0.010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b3baff3"
      },
      "source": [
        "!pip install box2d-py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 6. Plot Results\n",
        "# ===============================\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.plot(rewards_replay, label=\"With Replay & Target Network\")\n",
        "plt.plot(rewards_simple, label=\"Without Replay/Target\")\n",
        "plt.title(\"Episode Reward vs Episode Number\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.plot(epsilons_replay, label=\"With Replay & Target\")\n",
        "plt.plot(epsilons_simple, label=\"Without Replay/Target\")\n",
        "plt.title(\"Epsilon Decay over Time\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylabel(\"Epsilon\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.plot(losses_replay, label=\"With Replay & Target\")\n",
        "plt.plot(losses_simple, label=\"Without Replay/Target\")\n",
        "plt.title(\"Training Loss vs Steps\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kT9vsYTaxSA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 7. Observations\n",
        "# ===============================\n",
        "print(\"\"\"\n",
        "Observations:\n",
        "1. With experience replay and target network, training is much more stable.\n",
        "2. The epsilon value decays gradually, allowing a balance between exploration and exploitation.\n",
        "3. Without replay or target network, the reward curve is noisy and unstable.\n",
        "4. Replay memory helps break correlation between consecutive samples, improving convergence.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "0XUL6LRixS-e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}